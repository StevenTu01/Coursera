# Survey for Dynamic Neural network
Over the last decade several advances have been made in the paradigm of 
artificial neural networks with specific emphasis on architectures and learning algorithms.
it is envisaged that dynamic neural networks, 
in addition to better representation of biological neural systems,
offer better computational capabilities compared to their static counterparts.

## Research

[Dynamic Neural Networks: A Survey](https://arxiv.org/pdf/2102.04906.pdf) (Yizeng et al. 2021)

[Dynamic Neural Networks: An Overview](https://ieeexplore.ieee.org/document/854201) (Sinha & Gupta, 2000)

[Foundations and modelling of dynamic networks using Dynamic Graph Neural Networks: A survey](https://arxiv.org/abs/2005.07496) (Joakim et al. 2020)

[Temporal Networks](https://arxiv.org/abs/1108.1780) (Petter & Jari, 2011)

[Representation Learning for Dynamic Graphs: A Survey](https://arxiv.org/abs/1905.11485) (Seyed et al.2019)

[Community Discovery in Dynamic Networks: A Survey](https://arxiv.org/abs/1707.03186) (Giulio & Remy, 2017)

[Toward an interoperable dynamic network analysis toolkit](https://www.sciencedirect.com/science/article/abs/pii/S0167923606000601) (Kathleen et al. 2007)

[A deep learning approach to link prediction in dynamic networks](https://epubs.siam.org/doi/pdf/10.1137/1.9781611973440.33) (Xiaoyi et al. 2014)

[Deciding how to decide: Dynamic routing in artificial neural networks](https://arxiv.org/abs/1703.06217) (Mason & Pertro, 2017)

### Adaptive inference method

[Convolutional Networks with Adaptive Inference Graphs](https://arxiv.org/abs/1711.11503) (Veit & Belongie, 2018)

[Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification](https://arxiv.org/abs/2010.05300) (Yulin et al. 2020)

[Adaptive Mixtures of Local Experts](http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf) (Robert et al. 1991)

[Adaptive computation time for recurrent neural networks](https://arxiv.org/abs/1603.08983) [Alex, 2016]

[Depth-Adaptive Transformer](https://arxiv.org/abs/1910.10073) [Maha et al. 2020]

[FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/abs/2004.02178) [Weijie et al.2020]

[DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993) [Ji et al. 2020]

[BERT Loses Patience: Fast and Robust Inference with Early Exit](https://arxiv.org/abs/2006.04152) [Wangchunshun et al. 2020]

[Resolution Adaptive Networks for Efficient Inference](https://arxiv.org/abs/2003.07326) [Le et al. 2020]

[Multi-scale dense networks for resource efficient image classification](https://arxiv.org/abs/1703.09844) [Gao et al. 2017]

[Depth-Adaptive Transformer](https://arxiv.org/abs/1910.10073) [Maha et al.2020]

[Channel gating neural networks](https://arxiv.org/abs/1805.12549) [Weizhe et al. 2019]

[Branchynet: Fast inference via early exiting from deep neural networks](https://arxiv.org/abs/1709.01686) [Surat et al. 2017]

[Lifelong Learning with Dynamically Expandable Networks](https://arxiv.org/abs/1708.01547) [Jeongtae et al. 2018]

### Efficiency of Dynamic Neural Network

[Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877) (Jacob et al. 2018)

[CondConv: Conditionally Parameterized Convolutions for Efficient Inference](https://arxiv.org/abs/1904.04971) (Brandon et al. 2019)

[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) (Nitish et al. 2014)

### Compatibility

[Learning Dynamic Routing for Semantic Segmentation](https://arxiv.org/abs/2003.10401) (Yanwei et al. 2020)

[SkipNet: Learning Dynamic Routing in Convolutional Networks](https://arxiv.org/abs/1711.09485) (Wang et al. 2017)

[Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks](https://www.mitpressjournals.org/doi/pdf/10.1162/neco.1992.4.1.131) (Jurgen, 1992)

### Dynamic neural network with GNN

[Streaming Graph Neural Networks](https://arxiv.org/abs/1810.10627) (Yao et al. 2018)

[Predicting dynamic embedding trajectory in temporal interaction networks](https://arxiv.org/abs/1908.01207) (Srijan et al.2019)

[Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/abs/1812.08434) (Jie et al. 2019)

[Semi-supervised classification with graph convolutional networks](https://arxiv.org/abs/1609.02907) (Thomas & Max, 2017)

### Dynamic neural network with CNN
[ELASTIC: Improving CNNs with Dynamic Scaling Policies](https://arxiv.org/abs/1812.05262) [Huiyu et al. 2018]

[Language Modeling with Gated Convolutional Networks](http://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf) [Yann et al. 2017]

[Long Short-Term Memory](https://www.mitpressjournals.org/doi/pdf/10.1162/neco.1997.9.8.1735) [Sepp, 1997]

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) [Kaiming et al. 2016]

[A Deep Learning Framework for Dynamic Network Link Prediction](https://arxiv.org/abs/1902.08329) [Jinyin et al.2019]Â·

[Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale
Dependent Pooling and Cascaded Rejection Classifiers](http://users.umiacs.umd.edu/~fyang/papers/cvpr16.pdf) [Fan et al. 2020]

[Dynamic convolution: Attention over convolution kernels](https://arxiv.org/abs/1912.03458) (Yinpeng et al. 2020)

[DyNet: Dynamic Convolution for Accelerating Convolutional Neural Networks](https://arxiv.org/abs/2004.10694) (Yikang et al. 2020)

### Dynamic neural network with CV

[Dynamic computational time for visual attention](https://arxiv.org/abs/1703.10332) (Zhichao et al.2017)




