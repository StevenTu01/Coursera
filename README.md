# Survey for Dynamic Neural network
Over the last decade several advances have been made in the paradigm of 
artificial neural networks with specific emphasis on architectures and learning algorithms.
it is envisaged that dynamic neural networks, 
in addition to better representation of biological neural systems,
offer better computational capabilities compared to their static counterparts.

## Research

[Dynamic Neural Networks: A Survey](https://arxiv.org/pdf/2102.04906.pdf) (Yizeng et al. 2021)

[Dynamic Neural Networks: An Overview](https://ieeexplore.ieee.org/document/854201) (Sinha & Gupta, 2000)

[Foundations and modelling of dynamic networks using Dynamic Graph Neural Networks: A survey](https://arxiv.org/abs/2005.07496) (Joakim et al. 2020)

[Temporal Networks](https://arxiv.org/abs/1108.1780) (Petter & Jari, 2011)

[Representation Learning for Dynamic Graphs: A Survey](https://arxiv.org/abs/1905.11485) (Seyed et al.2019)

[Community Discovery in Dynamic Networks: A Survey](https://arxiv.org/abs/1707.03186) (Giulio & Remy, 2017)

### Adaptive inference method

[Convolutional Networks with Adaptive Inference Graphs](https://arxiv.org/abs/1711.11503) (Veit & Belongie, 2018)

[Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification](https://arxiv.org/abs/2010.05300) (Yulin et al. 2020)

[Adaptive Mixtures of Local Experts](http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf) (Robert et al. 1991)

[Adaptive computation time for recurrent neural networks](https://arxiv.org/abs/1603.08983) [Alex, 2016]

[Depth-Adaptive Transformer](https://arxiv.org/abs/1910.10073) [Maha et al. 2020]

[FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/abs/2004.02178) [Weijie et al.2020]

[DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993) [Ji et al. 2020]

[BERT Loses Patience: Fast and Robust Inference with Early Exit](https://arxiv.org/abs/2006.04152) [Wangchunshun et al. 2020]

[Resolution Adaptive Networks for Efficient Inference](https://arxiv.org/abs/2003.07326) [Le et al. 2020]

[Multi-scale dense networks for resource efficient image classification](https://arxiv.org/abs/1703.09844) [Gao et al. 2017]

### Efficiency of Dynamic Neural Network

[Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877) (Jacob et al. 2018)

[CondConv: Conditionally Parameterized Convolutions for Efficient Inference](https://arxiv.org/abs/1904.04971) (Brandon et al. 2019)

### Compatibility

[Learning Dynamic Routing for Semantic Segmentation](https://arxiv.org/abs/2003.10401) (Yanwei et al. 2020)

[SkipNet: Learning Dynamic Routing in Convolutional Networks](https://arxiv.org/abs/1711.09485) (Wang et al. 2017)

### Dynamic neural network with GNN

[Streaming Graph Neural Networks](https://arxiv.org/abs/1810.10627) (Yao et al. 2018)

[Predicting dynamic embedding trajectory in temporal interaction networks](https://arxiv.org/abs/1908.01207) (Srijan et al.2019)


